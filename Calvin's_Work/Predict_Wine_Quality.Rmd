---
title: "Logistic Regression"
author: "Calvin Skalla"
date: "3/20/2020"
output: 
  html_document: 
    self_contained: no
---

# Data Exploration and Cleaning
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(ggthemes)
library(cowplot)
library(plyr)
library(readr)
library(GGally)
library(mlbench)
library(caret)

#Reading in Data
white_wine <- read.csv("data/winequality-white.csv", sep = ";")
red_wine <- read.csv("data/winequality-red.csv", sep = ";")

#Creating Binomial Like Variable 1 = good, 0 = bad
WW_LR_data <- mutate(white_wine, like = as.factor(ifelse(quality >5, 1, 0))) %>% select(-quality) 

#Normalizing data with minmax
preproc2 <- preProcess(WW_LR_data, method = c("range"))
ww_data_minmax <- predict(preproc2, WW_LR_data)

#Splitting normalized data
w_train_index <- sample(1:nrow(ww_data_minmax), 0.8 * nrow(ww_data_minmax))
w_test_index <- setdiff(1:nrow(ww_data_minmax), w_train_index)
w_train <- ww_data_minmax[w_train_index, ]
w_test <- ww_data_minmax[w_test_index, ]
```

# Subset Selection w/ minmax data
```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(leaps)
regfit.full = regsubsets(like ~ ., ww_data_minmax, nvmax = 19)
reg.summary = summary(regfit.full)

which.min(reg.summary$bic)

```

# Forward and Backward Stepwise Selection
## Produced same results as Best Subset Selection for minimum BIC
```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
regfit.fwd = regsubsets(like ∼ ., data = WW_LR_data , nvmax = 11, method ="forward")
coef(regfit.fwd, 6)

regfit.bwd = regsubsets(like ∼ ., data = WW_LR_data , nvmax = 11, method ="backward")

coef(regfit.bwd, 6)

coef(regfit.full, 6)
```

# Cross Validation
```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
set.seed(1)
train = sample(c(1, 0), nrow(WW_LR_data), rep = TRUE)
test = (!train)



regfit.best = regsubsets(like ~ ., data = WW_LR_data[train,], nvmax = 11)
#summary(regfit.best)

test.mat = model.matrix(like ~ ., data = WW_LR_data[test,])

val.errors = rep(NA, 11)

#for(i in 1:11)
#{
  #coefi = coef(regfit.best, id = i)
  #pred = test.mat[, names(coefi)]%*%coefi
  #val.errors[i] = mean((pred - WW_LR_data[test,])^2)
  #print(val.errors)
#}
#val.errors
```


# First model 
### Used every feature available to build model while training on a 80% subset of the original dataset and tested on the remaining 20%
```{r, echo=FALSE, message=FALSE, warning=FALSE}
WW.LR.fit = glm(like ~ fixed.acidity + volatile.acidity + citric.acid
             + residual.sugar + chlorides + free.sulfur.dioxide 
             + total.sulfur.dioxide + density + pH + sulphates
             + alcohol, data = w_train, family = binomial)

#summary(WW.LR.fit)
#plot(WW.LR.fit)
WW.LR.probs = predict(WW.LR.fit, w_test, type = "response")
WW.LR.pred = rep(0, 980)
WW.LR.pred[WW.LR.probs>0.5] = 1
table(WW.LR.pred, w_test$like)
mean(WW.LR.pred==w_test$like)
```

# Second Model 
## First model shrunk w/ with significant variables only while training on a 80% subset of the original dataset and tested on the remaining 20%
```{r, echo=FALSE, message=FALSE, warning=FALSE}
LR.fit2 = glm(like ~ volatile.acidity + residual.sugar + free.sulfur.dioxide + 
                density + pH + sulphates + alcohol, data = w_train, family = binomial)

#summary(LR.fit2)
#plot(LR.fit2)
LR.probs2 = predict(LR.fit2, w_test, type = "response")
LR.pred2 = rep(0, 980)
LR.pred2[LR.probs2>0.5] = 1
table(LR.pred2, w_test$like)
mean(LR.pred2==w_test$like)
#0.74989 Initial probability trained with full dataset
```

# Third Model
## Second Model with Significant Variables minus Colinearity
```{r, echo=FALSE, message=FALSE, warning=FALSE}
LR.fit3 = glm(like ~ alcohol + free.sulfur.dioxide + volatile.acidity + 
                residual.sugar + pH + sulphates, data = w_train, family = binomial)

#summary(LR.fit3)
#plot(LR.fit3)
LR.probs3 = predict(LR.fit3, w_test, type = "response")
LR.pred3 = rep(0, 980)
LR.pred3[LR.probs3>0.5] = 1
table(LR.pred3, w_test$like)
mean(LR.pred3==w_test$like)

```

# Fourth Model
## Minimal BIC Model
## Same as Second Model
```{r, echo=FALSE, message=FALSE, warning=FALSE}
LR.BIC7 = glm(like ~ volatile.acidity + residual.sugar + free.sulfur.dioxide + 
                density + pH + sulphates + alcohol , data = w_train, family =
                binomial)
#summary(LR.fit4)
#plot(LR.fit4)
LR.probs4 = predict(LR.BIC7, w_test, type = "response")
LR.pred4 = rep(0, 980)
LR.pred4[LR.probs4>0.5] = 1
table(LR.pred4, w_test$like)
mean(LR.pred4==w_test$like)
```

# Fifth Model
## Second Lowest BIC Model
```{r, echo=FALSE, message=FALSE, warning=FALSE}
LR.BIC6 = glm(like ~ volatile.acidity + residual.sugar + density + pH + 
                sulphates + alcohol, data = w_train, family =binomial)
#summary(LR.fit4)
#plot(LR.fit4)
LR.probsBIC = predict(LR.BIC6, w_test, type = "response")
LR.predBIC = rep(0, 980)
LR.predBIC[LR.probsBIC>0.5] = 1
table(LR.predBIC, w_test$like)
mean(LR.predBIC==w_test$like)
```

# Sixth Model
## Handmade Model from Assumptions drawn from BIC graph
```{r, echo=FALSE, message=FALSE, warning=FALSE}
LR.fit5 = glm(like ~ volatile.acidity + residual.sugar + alcohol, data = w_train, 
              family = binomial)
#summary(LR.fit4)
#plot(LR.fit5)
LR.probs5 = predict(LR.fit5, w_test, type = "response")
LR.pred5 = rep(0, 980)
LR.pred5[LR.probs5>0.5] = 1
table(LR.pred5, w_test$like)
mean(LR.pred5==w_test$like)

```


